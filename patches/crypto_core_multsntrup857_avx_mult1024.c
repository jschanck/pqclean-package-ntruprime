--- supercop-20200826/crypto_core/multsntrup857/avx/mult1024.c	2020-08-31 19:56:55.531747281 -0400
+++ supercop-20200826-patched/crypto_core/multsntrup857/avx/mult1024.c	2020-08-31 20:09:19.735726664 -0400
@@ -15,6 +15,26 @@
 #define mulhrs_x16 _mm256_mulhrs_epi16
 #define signmask_x16(x) _mm256_srai_epi16((x),15)
 
+typedef union {
+  int16 v[512];
+  int16x16 _dummy;
+} vec512;
+
+typedef union {
+  int16 v[4][512];
+  int16x16 _dummy;
+} vec4x512;
+
+typedef union {
+  int16 v[1024];
+  int16x16 _dummy;
+} vec1024;
+
+typedef union {
+  int16 v[4*512];
+  int16x16 _dummy;
+} vec2048;
+
 static inline int16x16 squeeze_5167_x16(int16x16 x)
 {
   return sub_x16(x,mullo_x16(mulhrs_x16(x,const_x16(6)),const_x16(5167)));
@@ -92,9 +112,10 @@
     store_x16(&fpad[3][j],f3);
   }
 
-  for (i = 0;i < 4;++i)
+  for (i = 0;i < 4;++i) {
     for (j = 256;j < 512;++j)
       fpad[i][j] = 0;
+  }
 }
 
 static void unstride(int16 f[2048],const int16 fpad[4][512])
@@ -129,14 +150,74 @@
   }
 }
 
-#define ALIGNED __attribute((aligned(32)))
-
-static const ALIGNED int16 y_7681[512] = {
-#include "precomp7681.inc"
-} ;
-static const ALIGNED int16 y_10753[512] = {
-#include "precomp10753.inc"
-} ;
+static const vec512 y_7681 = { .v = {
+-3593,-617,-2804,3266,-2194,-1296,-1321,810,1414,3706,-549,-396,-121,-2088,-2555,1305,
+-3777,1921,103,3600,-2456,1483,1399,-1887,-1701,2006,1535,-3174,-2250,2816,-2440,-1760,
+-3625,2830,2043,-3689,1100,1525,-514,7,2876,-1599,3153,-1881,-2495,-2237,-2535,438,
+3182,3364,-1431,1738,3696,-2557,-2956,638,-2319,-1993,-2310,-3555,834,-1986,3772,-679,
+3593,617,2804,-3266,2194,1296,1321,-810,-1414,-3706,549,396,121,2088,2555,-1305,
+3777,-1921,-103,-3600,2456,-1483,-1399,1887,1701,-2006,-1535,3174,2250,-2816,2440,1760,
+3625,-2830,-2043,3689,-1100,-1525,514,-7,-2876,1599,-3153,1881,2495,2237,2535,-438,
+-3182,-3364,1431,-1738,-3696,2557,2956,-638,2319,1993,2310,3555,-834,1986,-3772,679,
+2665,727,-2572,2426,-2133,-1386,1681,-1054,2579,3750,373,3417,404,-2233,3135,-3405,
+-1799,1521,1497,-3831,-3480,-3428,2883,-1698,-859,-2762,2175,-194,-486,-3816,-1756,2385,
+-783,1533,3145,2,3310,-2743,2224,-1166,2649,-1390,3692,2789,1919,2835,-2391,-2732,
+1056,1464,1350,-915,-1168,-921,-3588,3456,-2160,-1598,730,2919,1532,-2764,-660,-2113,
+-2665,-727,2572,-2426,2133,1386,-1681,1054,-2579,-3750,-373,-3417,-404,2233,-3135,3405,
+1799,-1521,-1497,3831,3480,3428,-2883,1698,859,2762,-2175,194,486,3816,1756,-2385,
+783,-1533,-3145,-2,-3310,2743,-2224,1166,-2649,1390,-3692,-2789,-1919,-2835,2391,2732,
+-1056,-1464,-1350,915,1168,921,3588,-3456,2160,1598,-730,-2919,-1532,2764,660,2113,
+2005,-188,2345,-3723,-1403,2070,83,-3214,-3752,-1012,1837,-3208,3287,3335,-293,796,
+592,1519,-1338,1931,509,-2262,-3408,3334,3677,2130,642,589,-2167,-1084,-370,-3163,
+3763,-893,-2303,-402,2937,-1689,-1526,-3745,-2460,2874,2965,124,-1669,-1441,-3312,3781,
+2812,-2386,-2515,-429,-3343,777,-826,-3366,-3657,-1404,-791,-2963,-692,2532,2083,2258,
+-2005,188,-2345,3723,1403,-2070,-83,3214,3752,1012,-1837,3208,-3287,-3335,293,-796,
+-592,-1519,1338,-1931,-509,2262,3408,-3334,-3677,-2130,-642,-589,2167,1084,370,3163,
+-3763,893,2303,402,-2937,1689,1526,3745,2460,-2874,-2965,-124,1669,1441,3312,-3781,
+-2812,2386,2515,429,3343,-777,826,3366,3657,1404,791,2963,692,-2532,-2083,-2258,
+179,1121,2891,-3581,3177,-658,-3314,-1509,-17,151,2815,2786,1278,-2767,-1072,-1151,
+-1242,-2071,2340,-1586,2072,1476,2998,2918,-3744,-3794,-1295,451,-929,2378,-1144,434,
+-1070,-436,-3550,-3568,1649,715,3461,-1407,-2001,-1203,3770,1712,2230,-3542,2589,-3547,
+-2059,-236,3434,-3693,2161,-670,2719,2339,-2422,1181,3450,222,1348,-226,2247,-1779,
+-179,-1121,-2891,3581,-3177,658,3314,1509,17,-151,-2815,-2786,-1278,2767,1072,1151,
+1242,2071,-2340,1586,-2072,-1476,-2998,-2918,3744,3794,1295,-451,929,-2378,1144,-434,
+1070,436,3550,3568,-1649,-715,-3461,1407,2001,1203,-3770,-1712,-2230,3542,-2589,3547,
+2059,236,-3434,3693,-2161,670,-2719,-2339,2422,-1181,-3450,-222,-1348,226,-2247,1779,
+}} ;
+static const vec512 y_10753 = { .v = {
+1018,-1520,-2935,-4189,2413,918,4,1299,-2695,1341,-205,-4744,-3784,2629,2565,-3062,
+223,-4875,2790,-2576,-3686,-2503,3550,-3085,730,1931,-4513,4876,-3364,5213,2178,2984,
+4188,-4035,4129,-544,357,4347,1284,-2388,-4855,341,-1287,4102,425,5175,-4616,-4379,
+-3688,5063,3091,1085,-376,3012,-268,-1009,-2236,-3823,2982,-4742,-4544,-4095,193,847,
+-1018,1520,2935,4189,-2413,-918,-4,-1299,2695,-1341,205,4744,3784,-2629,-2565,3062,
+-223,4875,-2790,2576,3686,2503,-3550,3085,-730,-1931,4513,-4876,3364,-5213,-2178,-2984,
+-4188,4035,-4129,544,-357,-4347,-1284,2388,4855,-341,1287,-4102,-425,-5175,4616,4379,
+3688,-5063,-3091,-1085,376,-3012,268,1009,2236,3823,-2982,4742,4544,4095,-193,-847,
+-4734,4977,-400,-864,567,-5114,-4286,635,512,-1356,-779,-2973,675,-5064,-1006,1268,
+2998,2981,-151,-3337,3198,-909,2737,-970,2774,886,2206,1324,2271,454,-326,-3715,
+-3441,-4580,636,2234,-794,3615,578,-472,3057,-5156,-2740,2684,1615,-1841,-336,-1586,
+5341,-116,5294,4123,5023,-1458,-3169,467,-2045,4828,-1572,-5116,-2213,-4808,2884,1068,
+4734,-4977,400,864,-567,5114,4286,-635,-512,1356,779,2973,-675,5064,1006,-1268,
+-2998,-2981,151,3337,-3198,909,-2737,970,-2774,-886,-2206,-1324,-2271,-454,326,3715,
+3441,4580,-636,-2234,794,-3615,-578,472,-3057,5156,2740,-2684,-1615,1841,336,1586,
+-5341,116,-5294,-4123,-5023,1458,3169,-467,2045,-4828,1572,5116,2213,4808,-2884,-1068,
+3453,2196,2118,5005,2428,-2062,-1930,2283,4601,3524,-3241,-1409,-2230,-5015,4359,4254,
+5309,2657,-2050,-4428,4250,-2015,-3148,-778,2624,-1573,40,2237,-573,-4447,2909,1122,
+854,-4782,2439,4408,5172,4784,4144,1639,3760,2139,2680,-663,4621,3135,1349,-97,
+5215,3410,-2117,-1992,-1381,-1635,274,-2419,3570,458,2087,-2374,-1132,2662,-1722,5313,
+-3453,-2196,-2118,-5005,-2428,2062,1930,-2283,-4601,-3524,3241,1409,2230,5015,-4359,-4254,
+-5309,-2657,2050,4428,-4250,2015,3148,778,-2624,1573,-40,-2237,573,4447,-2909,-1122,
+-854,4782,-2439,-4408,-5172,-4784,-4144,-1639,-3760,-2139,-2680,663,-4621,-3135,-1349,97,
+-5215,-3410,2117,1992,1381,1635,-274,2419,-3570,-458,-2087,2374,1132,-2662,1722,-5313,
+-2487,-554,4519,2449,73,3419,624,-1663,-1053,4889,279,1893,1111,1510,2279,-4540,
+2529,2963,5120,-3995,-5107,-3360,-5356,2625,-4403,152,-5083,-2807,2113,-4000,-4328,3125,
+-2605,4967,-1056,1160,1927,693,-4003,3827,-4670,-569,3535,-5268,1782,825,355,5068,
+5334,4859,-1689,-2788,-4891,-3260,1204,3891,-4720,-4973,2813,2205,834,-4393,-2151,3096,
+2487,554,-4519,-2449,-73,-3419,-624,1663,1053,-4889,-279,-1893,-1111,-1510,-2279,4540,
+-2529,-2963,-5120,3995,5107,3360,5356,-2625,4403,-152,5083,2807,-2113,4000,4328,-3125,
+2605,-4967,1056,-1160,-1927,-693,4003,-3827,4670,569,-3535,5268,-1782,-825,-355,-5068,
+-5334,-4859,1689,2788,4891,3260,-1204,-3891,4720,4973,-2813,-2205,-834,4393,2151,-3096,
+}} ;
 /*
   can also compute these on the fly, and share storage,
   at expense of 2 NTTs on top of the 24 NTTs below:
@@ -152,11 +233,13 @@
 
 static void mult1024(int16 h[2048],const int16 f[1024],const int16 g[1024])
 {
-  ALIGNED int16 fpad[4][512];
-  ALIGNED int16 gpad[4][512];
-  ALIGNED int16 h_7681[2048];
-  ALIGNED int16 h_10753[2048];
+  vec4x512 x1, x2;
+  vec2048 x3, x4;
+#define fpad (x1.v)
+#define gpad (x2.v)
 #define hpad fpad
+#define h_7681 (x3.v)
+#define h_10753 (x4.v)
   int i;
 
   stride(fpad,f);
@@ -196,7 +279,7 @@
     int16x16 h4 = sub_x16(d1d2d3,e13);
     int16x16 h5 = sub_x16(d2d3,e23);
     int16x16 h6 = d3;
-    int16x16 twist = load_x16(&y_7681[i]);
+    int16x16 twist = load_x16(&y_7681.v[i]);
     h4 = mulmod_7681_x16(h4,twist);
     h5 = mulmod_7681_x16(h5,twist);
     h6 = mulmod_7681_x16(h6,twist);
@@ -210,7 +293,7 @@
   }
 
   invntt512_7681(hpad[0],4);
-  unstride(h_7681,hpad);
+  unstride(h_7681,(const int16(*)[512]) hpad);
 
   stride(fpad,f);
   ntt512_10753(fpad[0],4);
@@ -249,7 +332,7 @@
     int16x16 h4 = sub_x16(d1d2d3,e13);
     int16x16 h5 = sub_x16(d2d3,e23);
     int16x16 h6 = d3;
-    int16x16 twist = load_x16(&y_10753[i]);
+    int16x16 twist = load_x16(&y_10753.v[i]);
     h4 = mulmod_10753_x16(h4,twist);
     h5 = mulmod_10753_x16(h5,twist);
     h6 = mulmod_10753_x16(h6,twist);
@@ -263,7 +346,7 @@
   }
 
   invntt512_10753(hpad[0],4);
-  unstride(h_10753,hpad);
+  unstride(h_10753,(const int16(*)[512]) hpad);
 
   for (i = 0;i < 2048;i += 16) {
     int16x16 u1 = load_x16(&h_10753[i]);
@@ -298,9 +381,11 @@
 
 int crypto_core(unsigned char *outbytes,const unsigned char *inbytes,const unsigned char *kbytes,const unsigned char *cbytes)
 {
-  ALIGNED int16 f[1024];
-  ALIGNED int16 g[1024];
-  ALIGNED int16 fg[2048];
+  vec1024 x1, x2;
+  vec2048 x3;
+#define f (x1.v)
+#define g (x2.v)
+#define fg (x3.v)
 #define h f
   int i;
   int16x16 x;
@@ -317,9 +402,9 @@
     store_x16(&f[i],x);
   }
   for (i = 0;i < p;++i) {
-    int8 gi = kbytes[i];
+    int8 gi = (int8) kbytes[i];
     int8 gi0 = gi&1;
-    g[i] = gi0-(gi&(gi0<<1));
+    g[i] = (int8) (gi0-(gi&(gi0<<1)));
   }
 
   mult1024(fg,f,g);
diff -ru --no-dereference supercop-20200826/crypto_core/multsntrup857/avx/ntt.c supercop-20200826-patched/crypto_core/multsntrup857/avx/ntt.c
